---
title: Replication of 'Why Do People Tend to Infer Ought From Is? The Role of Biases
  in Explanation' by Tworek & Cimpian (2016, Psychological Science)
author: "Marianna Zhang (marianna.zhang@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    toc_depth: '3'
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

##Introduction

<!-- [No abstract is needed.]  Each replication project will have a straightforward, no frills report of the study and results.  These reports will be publicly available as supplementary material for the aggregate report(s) of the project as a whole.  Also, to maximize project integrity, the intro and methods will be written and critiqued in advance of data collection.  Introductions can be just 1-2 paragraphs clarifying the main idea of the original study, the target finding for replication, and any other essential information.  It will NOT have a literature review -- that is in the original publication. You can write both the introduction and the methods in past tense.-->

<!-- A short justification for your choice of experiment in terms of your research interests or research program (1 paragraph). This justification should tell us why you chose this particular result.-->

Tworek & Cimpian 2016 is a paper that investigates the is-ought fallacy by exploring how people tend to explain what is common (e.g. Roses are commonly gifted on Valentine's Day) in terms of intrinsic properties (e.g. Roses are beautiful), which then promotes normative conclusions about what is good (e.g. It is good to give roses). This paper is relevant to my interest in how we represent categories and how we develop those category representations, including how we represent descriptive and normative dimensions of categories and to what extent those dimensions are separate and to what extent they interact. Tworek & Cimpian 2016 addresses this topic by investigating how our normative beliefs about categories can emerge from descriptive properties of categories. From replicating this paper, I hope to both learn further about a topic relevant to my research area and develop skills in running the kinds of studies used in the course of that research. 

<!-- A description of the stimuli and procedures that will be required to conduct this experiment, and what the challenges will be (1-2 paragraphs).-->

Tworek & Cimpian Experiment 1 shows that individuals' preference for intrinsic explanations of common phenomena correlates with their endorsement of normative judgments about common phenomena. 

Adults on Mechanical Turk were administered a task that assessed their preference for inherent explanatiosn of common phenomena (inherence bias measure) and a task that assessed their endorsement of normative judgments for common phenomena (ought measure), in random order. The inherence bias task consisted of rating agreement with 15 sentences that proposed inherent explanations for common phenomena (e.g. "Black is associated with funerals because of something about the color black or about funerals-maybe because the darkness of black conveys how people feel at funerals"). The ought task consisted of reading 6 passages written like press releases that described a common phenomenon (e.g. the popularity of eating pizza among Americans), and then for each passage, answering an ought question about that phenomenon that either used the term "should" or used the term "good" (e.g. "Do you think it should be that so many Americans eat pizza?"), along with 4 filler questions. Finally, participants provided demographics information and were debriefed. The critical analyses was to show that participants' inherence bias measure was positively correlated with their ought measure. In other words, participants' preference for inherence explanations for common phenomena is correlated with their likelihood of endorsing normative ought judgments of common phenomena.

The study will be easy to adminster and run, since it involves surveys administered on MTurk. 

The [repository for the replication](https://github.com/mariannazhang/tworek2016) and the [original paper](https://github.com/mariannazhang/tworek2016/blob/master/original_paper.pdf) can be found online. 


##Methods

###Power Analysis

The original effect size was r=0.30. For 80% power, this effect size requires n=82. 

This sample size is smaller than the original sample size of n=122, which achieved a post-hoc power of 93%.

###Planned Sample

<!-- Planned sample size and/or termination rule, sampling frame, known demographics if any, preselection rules if any.-->

Planned sample size is 82 adults recruited on Amazon Mechanical Turk. Participants will be required to be located within the United States, and will be required to have a HIT acceptance rate of 80% or above.

###Materials
<!-- All materials - can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article.-->

The following materials were used:

"*Ought inferences.* Participants read six passages that were structured like and derived from actual press releases. The passages described a typical societal practice (i.e., what is). For example, one was titled "America's Pizza Obsession: By The Numbers" and read as follows:

>The quintessential American food may be apple pie, but its popularity pales beside our national love affair with pizza pies. The Daily reports that Americans consume a staggering 100 acres of pizza a day, according to data from the National Association of Pizza Operators (NAPO). Over $38 billion of pizza is sold in America annually, according to Pizza Today, and 3 billion pizzas are sold in the U.S. each year according to NAPO. 350 slices of pizza are sold every second, according to NAPO, and the average American eats an average of 46 slices of pizza a year, according to Packaged Facts. Overall, a total of 94% of Americans eat pizza (adapted from "America's pizza obsession: By the numbers," 2011). 

After reading each press release, participants were asked five questions: one ought question (e.g., "Do you think it should be that so many Americans eat pizza?" 1 = definitely no, 9 = definitely yes) and four filler questions that served to camouflage the main focus of the study (e.g., "Do you think the amount of pizza sold will grow in the next 5 years?" "What do you think accounts for the current prices of pizza?"). For three of the press releases, the ought question was phrased with "should" (see the example in the first sentence of this paragraph), and for the other three, the ought questions were phrased with "good" - for example, "Do you think that it's good that so many Americans drive to work?" (1 = really not good, 9 = really good), which was presented after a passage claiming that 88% of Americans drive to work. Participants' average scores for the "good" and "should" questions were significantly correlated, r(122) = .37, 95% confidence interval (CI) = [.20, .51], p < .001. 

Note that the press releases were purposely about behaviors that fall outside the scope of most existing accounts of sociomoral reasoning (eating pizza, driving to work, drinking coffee, owning a TV, using e-mail, and watching football) so that our results would highlight the unique contribution of our account. All passages were factual in tone, without evaluative language, to avoid influencing participants' normative judgments (for the full text of the passages, see the Supplemental Material at Open Science Framework, https://osf.io/4kanr/).

Responses to the six ought questions were averaged into a composite score, which we refer to as the ought measure (\alpha = .58). The lowest correlation between a particular question and the average of all six questions (i.e., the lowest item-total correlation) was .33. (Note that the results remained the same when excluding the item with the lowest item-total correlation.) The ought measure served as our main dependent variable.

*Inherence bias.* Fifteen items were used to assess the extent to which participants preferred explanations in terms of inherent facts (e.g., "Black is associated with funerals because of something about the color black or about funerals - maybe because the darkness of black conveys how people feel at funerals"; \alpha = .85; lowest item total correlation = .47; see Table 1 for other sample items). All items were rated using a 9-point scale (1 = disagree strongly, 9 = agree strongly) and were presented in random order. Note that, as with the ought measure, the items in the measure of inherence bias were worded factually and did not contain evaluative language. Two catch items were included to detect inattention (e.g., "Please click on the number three below to indicate that you are paying attention"). Participants who missed either of these attention checks were excluded (n = 7).

*Control measures.* Four control measures were administered to investigate alternative explanations for the predicted relationship between participants' explanations and their ought inferences. These measures tapped into dimensions that could influence both variables of interest, giving rise to a correlation between them in the absence of a causal relationship. First, we measured participants' level of education using a scale from 1, less than high school, to 6, doctoral (Ph.D., J.D., M.D.). Second, we measured their fluid intelligence with one 12-item set of Raven's Progressive Matrices (Raven, 1960; see also Salomon & Cimpian, 2014). Third, we measured participants' political views: "How would you describe your political attitudes?" (1 = strongly liberal, 9 = strongly conservative). (Because higher scores on this measure indicate more conservative attitudes, we occasionally refer to it as a measure of conservatism.)

Fourth, a measure related to the measure of political views assessed participants' belief in a just world: for example, "Basically, the world is a just place" (1 = disagree strongly, 9 = agree strongly; Rubin & Peplau, 1975). Table S1 in the Supplemental Material (at Open Science Framework, https://osf.io/4kanr/) provides descriptive statistics for these measures."


Two control measures - belief-in-a-just-world scale, Raven's Progressive Matrices - were omitted for reasons of study duration and costs, and since the original study found a significant correlation between inherence bias measure and ought measure even without controlling for any of the four control measures. 

###Procedure	

<!-- Can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article.-->

The following procedure was followed, with exceptions noted below:

"Participants were tested online via Qualtrics (Qualtrics Labs Inc., Provo, UT). The ought measure, the measure of inherence bias, the belief-in-a-just-world scale, and Raven's Progressive Matrices were presented in random order. Item order was randomized for all scales except Raven's Progressive Matrices, which were presented in increasing order of difficulty. The measures of participants' education and conservatism were administered at the end of the sessions, along with other demographic questions. Finally, participants were debriefed."

The ought measure and inherence bias were presented in random order. Two control measures - belief-in-a-just-world scale and fluid intelligence (a 12-item set of Raven's Progressive Matrices) - were omitted for reasons of study duration and costs, and since the original study found a significant correlation between inherence bias measure and ought measure even without controlling for any of the four control measures. In demographics, participants were asked for their age instead of DOB out of concerns about identifiable information.

The survey paradigm, adapted from the original survey paradigm, can be found [on Qualtrics](https://stanforduniversity.qualtrics.com/jfe/form/SV_298GpHbVEYbI5GB). 

###Analysis Plan

<!-- Can also quote directly, though it is less often spelled out effectively for an analysis strategy section.  The key is to report an analysis strategy that is as close to the original - data cleaning rules, data exclusion rules, covariates, etc. - as possible.  

**Clarify key analysis of interest here**  You can also pre-specify additional analyses you plan to do.-->

####Exclusion criteria
Participants will be excluded if they do not provide the answer they are told to provide to any of the two catch items on the Inherence Heuristic Scale manipulation catch, or if they indicate during debriefing that they had not paid attention. 

####Analysis of interest
A Pearson correlation will be derived between subjects' inherence bias measure and ought measure. The Pearson correlation is expected to be positive and statistically significant (p<0.05).


###Differences from Original Study

<!-- Explicitly describe known differences in sample, setting, procedure, and analysis plan from original study.  The goal, of course, is to minimize those differences, but differences will inevitably occur.  Also, note whether such differences are anticipated to make a difference based on claims in the original article or subsequent published research on the conditions for obtaining the effect.-->

Two control measures, fluid intelligence (Raven's Progressive Matrices) and belief-in-a-just-world, were omitted out of cost concerns. The final analysis in this study finds the Pearson correlation between subjects' inherence bias measure and ought measure, without controlling for any control measures. Given that the original study reported a significant correlation without controlling for any control measures, as well as a significant correlation with controlling for control measures, this analysis plan is also expected to find a significant correlation without controlling for any control measures. Controlling for control measures should increase the signal, so failing to control for the control measures should merely add noise.

The sample size is smaller (n=82) than the original (n=122) to achieve 80% power if the effect size is the same as what they found.

In addition, the original study paid its Amazon Mechanical Turk participants 75 cents for completing the study. In this replication, participants will be paid 1.25, which is the current federal minimum wage for the expected time to completion. This increase in subject payment is not predicted to affect the result of the replication, since the increase is a trivial amount. 

Finally, in the demographics section, participants are asked for their age instead of their date of birth out of concerns about identifiable information. This change should have no affect on the result, since demographics is the final task and no analysis was performed on demographic information.

<!-- 
### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.
-->

##Results


### Data preparation

Data preparation following the analysis plan.

```{r "knitr config", cache = FALSE}
require("knitr")
```


```{r}
### Data Preparation


#### Load Relevant Libraries and Functions
library(tidyverse)



#### Import data for analysis
# pilot A = self (included because pilot B data is insufficient for analysis to run)
data_A <- read_csv("../data/Exp1_pilotA.csv")
# pilot B = naive participants (n=3)
data_B <- read_csv("../data/Exp1_pilotB.csv")


#### Initial data formatting
# combine data
data_A <- mutate(data_A, dataset = "pilotA")
data_B <- mutate(data_B, dataset = "pilotB")

data <- bind_rows(data_A, data_B)


# Filter out those who didn't consent
data <- data %>%
  filter(consent == "I agree")


# Select relevant columns
data <- data %>%
  dplyr::select(IH_1:IH_C2, # inherence heuristic scale
         O_1, O_2, O_3, O_4, O_5, O_6, # ought inferences
         attn, # attention check
         debrief_1:debrief_4, 
         dataset) # debriefing

# Add subject ID
data <- mutate(data, subject = row_number())

# Remove all strings in task response columns (mainly Likert scale labels)
data <- data %>% 
  mutate_at(vars(IH_1:O_6), ~gsub("([a-z]|[A-Z]|\\s)*", "", .)) %>% 
  mutate_at(vars(IH_1:O_6), as.numeric)



#### Data exclusion / filtering
## Record exclusions

# replace NAs in debriefing questions with text " " to prep for str_detect, which can't handle NAs
data$debrief_1 <- str_replace_na(data$debrief_1, " ")
data$debrief_2 <- str_replace_na(data$debrief_2, " ")
data$debrief_3 <- str_replace_na(data$debrief_1, " ")
data$debrief_4 <- str_replace_na(data$debrief_1, " ")

# record exclusions
data_excl <- tibble(
  attn = sum(data$attn != "Yes" | is.na(data$attn)), 
  IH_C = sum(data$IH_C1 != 3 | is.na(data$IH_C1)) 
  + sum(data$IH_C2 != 7 | is.na(data$IH_C2)),
  non_naive = sum(str_detect(data$debrief_1, "is-ought"))
  + sum(str_detect(data$debrief_2, "is-ought"))
  + sum(str_detect(data$debrief_3, "is-ought"))
  + sum(str_detect(data$debrief_4, "is-ought")))
data_excl

## Excluding subjects
data <- data %>%
  filter(attn == "Yes" & # Exclude those who didn't pay attention
        IH_C1 == 3 & IH_C2 == 7 & # Exclude those who didn't answer correctly on control questions on the inherence heuristic scale
        !str_detect(debrief_1, "is-ought") & 
        !str_detect(debrief_2, "is-ought") & 
        !str_detect(debrief_3, "is-ought") & 
        !str_detect(debrief_4, "is-ought")) %>%  # Exclude those who mentioned "is-ought" in debriefing
  select(-attn, -(IH_C1:IH_C2), -(debrief_1:debrief_4)) # Delete attn and inherence heuristic scale control columns



#### Prepare data for analysis - create columns etc.
# gather to tidy long form
data_tidy <- data %>% 
  gather(question, response, IH_1:O_6)

# classify items as inherence heuristic questions vs ought questions
data_tidy <- data_tidy %>% 
  separate(question, c("question_type", "question_number"), "_")

# summarize inherence bias average and ought measure average across subjects
data_means <- data_tidy %>%
  group_by(question_type) %>% 
  summarize(avg=mean(response, na.rm=TRUE), sd=sd(response, na.rm=TRUE), n())
data_means

# summarize inherence bias and ought measure per subject
data_means_subj <- data_tidy %>% 
  group_by(subject, question_type, dataset) %>%  
  summarize(avg=mean(response, na.rm=TRUE)) %>% 
  spread(question_type, avg)
data_means_subj






# A short analysis of participant demographics. Participant demographics may be a big reason why you get the result you do, so it'll be important for readers to know about them. 

```

### Confirmatory analysis

The analyses as specified in the analysis plan. 

The original result was r(120) = .30, 95% CI = [.13, .46], p < .001.

```{r}
# If time - scatterplot of inherence bias measure and ought measure in original data



# scatterplot of inherence bias measure and ought measure
ggplot(data_means_subj, aes(x = IH, y = O)) + 
  geom_point() + 
  labs(x="Inherence bias measure", 
       y="Ought measure") +
  geom_smooth(method = "lm") +
  # Ought measure is 1 to 9 scale
  scale_y_continuous(limits=c(1,9), breaks=seq(1, 9, 1)) +
  # Inherence bias measure is 1 to 9 scale
  scale_x_continuous(limits=c(1,9), breaks=seq(1, 9, 1)) +
  ggtitle("Inherence bias measure is correlated with ought measure")


# Pearson correlation between each subject's inherence bias measure and their ought measure 
cor <- cor.test(data_means_subj$IH, data_means_subj$O, method = "pearson")
cor




# difference between this correlation and original study's correlation (r(120) = .30, 95% CI = [.13, .46], p < .001)
diff_cor <- cor$estimate - 0.30
diff_cor





```
The result in this replication was r(`r cor$parameter`) = `r round(cor$estimate, 2)`, 95% CI = [`r round(cor$conf.int[1], 2)`, `r round(cor$conf.int[2], 2)`], p = `r cor$p.value`.

###Exploratory analyses

<!-- Any follow-up analyses desired (not required). --> 

## Discussion

### Summary of Replication Attempt

<!-- Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.-->

### Commentary

<!-- Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.-->
